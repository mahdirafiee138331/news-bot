# -*- coding: utf-8 -*-
import os
import logging
import json
import re
import requests
import time
import feedparser
import google.generativeai as genai
import html as html_lib # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡ html Ø¨Ø±Ø§ÛŒ Ø§Ù…Ù†ÛŒØª Ø¨ÛŒØ´ØªØ±

# --- Ø®ÙˆØ§Ù†Ø¯Ù† Ù…ØªØºÛŒØ±Ù‡Ø§ÛŒ Ù…Ø­Ø±Ù…Ø§Ù†Ù‡ ---
TELEGRAM_BOT_TOKEN = os.environ.get("TELEGRAM_BOT_TOKEN")
ADMIN_CHAT_ID = os.environ.get("ADMIN_CHAT_ID")
GEMINI_API_KEY = os.environ.get("GEMINI_API_KEY")
ADMIN_NAME = os.environ.get("ADMIN_NAME", "Ø¬Ù†Ø§Ø¨ Ø±ÙÛŒØ¹ÛŒ")

# --- Ù¾ÛŒÚ©Ø±Ø¨Ù†Ø¯ÛŒ Ø¬Ù…ÛŒÙ†Ø§ÛŒ ---
model = None
if GEMINI_API_KEY:
    try:
        genai.configure(api_key=GEMINI_API_KEY)
        model = genai.GenerativeModel('gemini-pro')
    except Exception as e:
        logging.error(f"Ø®Ø·Ø§ Ø¯Ø± Ù¾ÛŒÚ©Ø±Ø¨Ù†Ø¯ÛŒ Ø¬Ù…ÛŒÙ†Ø§ÛŒ: {e}")

# --- Ù…Ø³ÛŒØ± ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ ---
DB_FILE = "bot_database.json"
URL_FILE = "urls.txt"

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')

KEYWORD_CATEGORIES = {
    "ğŸ”µ": ['Ù†Ø¬ÙˆÙ…', 'ÙÛŒØ²ÛŒÚ©', 'Ú©ÛŒÙ‡Ø§Ù†', 'Ú©ÙˆØ§Ù†ØªÙˆÙ…', 'Ø³ØªØ§Ø±Ù‡', 'Ú©Ù‡Ú©Ø´Ø§Ù†', 'Ø³ÛŒØ§Ù‡Ú†Ø§Ù„Ù‡', 'Ø§Ø®ØªØ±Ø´Ù†Ø§Ø³ÛŒ', 'Ø³ÛŒØ§Ø±Ù‡', 'physics', 'astronomy', 'cosmos', 'galaxy', 'planet'],
    "ğŸŸ¡": ['Ø²ÛŒØ³Øª', 'Ú˜Ù†ØªÛŒÚ©', 'ÙØ±Ú¯Ø´Øª', 'dna', 'Ø³Ù„ÙˆÙ„', 'Ù…ÙˆÙ„Ú©ÙˆÙ„', 'Ø¨ÛŒÙˆÙ„ÙˆÚ˜ÛŒ', 'ØªÚ©Ø§Ù…Ù„', 'biology', 'evolution', 'genetic'],
    "âš«": ['Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ', 'ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ù…Ø§Ø´ÛŒÙ†', 'Ø´Ø¨Ú©Ù‡ Ø¹ØµØ¨ÛŒ', 'Ø±Ø¨Ø§ØªÛŒÚ©', 'Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ…', 'Ø¯ÛŒÙ¾ Ù„Ø±Ù†ÛŒÙ†Ú¯', 'ai', 'artificial intelligence', 'machine learning'],
    "ğŸ”´": ['Ø±ÙˆØ§Ù†Ø´Ù†Ø§Ø³ÛŒ', 'Ø¬Ø§Ù…Ø¹Ù‡ Ø´Ù†Ø§Ø³ÛŒ', 'Ø¹Ù„ÙˆÙ… Ø§Ø¬ØªÙ…Ø§Ø¹ÛŒ', 'Ø±ÙØªØ§Ø±', 'Ø°Ù‡Ù†', 'Ø±ÙˆØ§Ù†', 'Ø§Ø¬ØªÙ…Ø§Ø¹ÛŒ', 'psychology', 'sociology', 'social'],
    "ğŸŸ ": ['ÙÙ„Ø³ÙÙ‡', 'ÙÙ„Ø³ÙÙ‡ Ø¹Ù„Ù…', 'Ù…Ù†Ø·Ù‚', 'Ù…ØªØ§ÙÛŒØ²ÛŒÚ©', 'Ø§Ø®Ù„Ø§Ù‚', 'philosophy']
}

def load_data():
    if os.path.exists(DB_FILE):
        try:
            with open(DB_FILE, 'r', encoding='utf-8') as f:
                return json.load(f)
        except (json.JSONDecodeError, FileNotFoundError):
            logging.warning("ÙØ§ÛŒÙ„ Ø¯ÛŒØªØ§Ø¨ÛŒØ³ Ø®Ø±Ø§Ø¨ ÛŒØ§ Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯. ÛŒÚ© ÙØ§ÛŒÙ„ Ø¬Ø¯ÛŒØ¯ Ø³Ø§Ø®ØªÙ‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯.")
            return {"last_sent_links": {}}
    return {"last_sent_links": {}}

def save_data(data):
    try:
        with open(DB_FILE, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=4)
    except Exception as e:
        logging.error(f"Ø®Ø·Ø§ Ø¯Ø± Ø°Ø®ÛŒØ±Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§: {e}")

def clean_html(raw_html):
    return re.sub(re.compile('<.*?>'), '', raw_html or "")

def categorize_article(text):
    text_lower = (text or "").lower()
    emojis = ""
    for emoji, keywords in KEYWORD_CATEGORIES.items():
        if any(keyword in text_lower for keyword in keywords):
            emojis += emoji
    return emojis if emojis else "ğŸ“°"

def send_telegram_message(text):
    if not TELEGRAM_BOT_TOKEN or not ADMIN_CHAT_ID:
        logging.error("ØªÙˆÚ©Ù† ØªÙ„Ú¯Ø±Ø§Ù… ÛŒØ§ Ø´Ù†Ø§Ø³Ù‡ Ø§Ø¯Ù…ÛŒÙ† ØªÙ†Ø¸ÛŒÙ… Ù†Ø´Ø¯Ù‡ Ø§Ø³Øª.")
        return False

    send_url = f"https://api.telegram.org/bot{TELEGRAM_BOT_TOKEN}/sendMessage"
    payload = {
        "chat_id": ADMIN_CHAT_ID,
        "text": text,
        "parse_mode": "HTML",
    }
    try:
        r = requests.post(send_url, json=payload, timeout=15)
        logging.info("Telegram send status: %s", r.status_code)
        logging.info("Telegram response: %s", r.text)
        r.raise_for_status()
        return True
    except Exception as e:
        logging.error(f"Ø®Ø·Ø§ Ø¯Ø± Ø§Ø±Ø³Ø§Ù„ Ù¾ÛŒØ§Ù… ØªÙ„Ú¯Ø±Ø§Ù…: {e}")
        return False

def process_with_gemini(title, summary):
    if model is None:
        logging.warning("Ù…Ø¯Ù„ Ø¬Ù…ÛŒÙ†Ø§ÛŒ Ù…Ù‚Ø¯Ø§Ø±Ø¯Ù‡ÛŒ Ù†Ø´Ø¯Ù‡Ø› Ø®Ø±ÙˆØ¬ÛŒ fallback Ø§Ø±Ø³Ø§Ù„ Ù…ÛŒâ€ŒØ´ÙˆØ¯.")
        return f"<b>{html_lib.escape(title)}</b>\n\n(Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¨Ø§ Ø¬Ù…ÛŒÙ†Ø§ÛŒ Ø¯Ø± Ø¯Ø³ØªØ±Ø³ Ù†ÛŒØ³Øª)"

    try:
        prompt = (
            "You are an expert science communicator. Your task is to perform two steps based on the following English text:\n"
            "1. Provide a fluent and professional Persian translation of ONLY the title.\n"
            "2. After the title, explain the core concept of the article in a few simple and conceptual Persian sentences, as if you are explaining it to an expert student (like Ø´Ø§Ú¯Ø±Ø¯Ù… in Persian).\n\n"
            f"Title: '{title}'\n"
            f"Summary: '{summary}'\n\n"
            "Your final output must be in Persian and structured exactly like this:\n"
            "[Persian Title]\n\n"
            "[Simple and conceptual Persian explanation]"
        )
        response = model.generate_content(prompt)
        return response.text
    except Exception as e:
        logging.error(f"Ø®Ø·Ø§ Ø¯Ø± Ø§Ø±ØªØ¨Ø§Ø· Ø¨Ø§ Ø¬Ù…ÛŒÙ†Ø§ÛŒ: {e}")
        return f"<b>{html_lib.escape(title)}</b>\n\n(Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¨Ø§ Ø¬Ù…ÛŒÙ†Ø§ÛŒ Ù†Ø§Ù…ÙˆÙÙ‚ Ø¨ÙˆØ¯)"

def check_news_job():
    database = load_data()
    last_sent_links = database.get("last_sent_links", {})
    
    logging.info("Ø´Ø±ÙˆØ¹ Ú†Ø±Ø®Ù‡ Ø¨Ø±Ø±Ø³ÛŒ Ø§Ø®Ø¨Ø§Ø±...")
    try:
        with open(URL_FILE, 'r', encoding='utf-8') as f:
            urls = [line.strip() for line in f if line.strip() and not line.startswith('#')]
    except FileNotFoundError:
        logging.warning("ÙØ§ÛŒÙ„ urls.txt Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯!")
        urls = []

    for url in urls:
        logging.info(f"Ø¯Ø± Ø­Ø§Ù„ Ø¨Ø±Ø±Ø³ÛŒ Ø³Ø§ÛŒØª: {url}")
        try:
            feed = feedparser.parse(url)
            if not feed or not feed.entries:
                logging.warning(f"ÙÛŒØ¯ Ø¨Ø±Ø§ÛŒ Ø³Ø§ÛŒØª {url} Ø®Ø§Ù„ÛŒ ÛŒØ§ Ù†Ø§Ù…Ø¹ØªØ¨Ø± Ø§Ø³Øª.")
                continue
            
            for entry in reversed(feed.entries[:15]):
                entry_id = entry.get('id', entry.link)
                if last_sent_links.get(url) == entry_id:
                    continue
                
                title = entry.get("title", "(Ø¨Ø¯ÙˆÙ† Ø¹Ù†ÙˆØ§Ù†)")
                summary = clean_html(entry.get("summary", "") or entry.get("description", ""))
                full_content_for_cat = f"Title: {title}. Summary: {summary}"
                emojis = categorize_article(full_content_for_cat)
                
                gemini_output_raw = process_with_gemini(title, summary)
                # Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù…ØªÙ† Ø¨Ø±Ø§ÛŒ Ø§Ø±Ø³Ø§Ù„ Ø¨Ù‡ ØµÙˆØ±Øª HTML
                gemini_output_escaped = html_lib.escape(gemini_output_raw)
                message_part = f"{emojis} <b>{gemini_output_escaped.splitlines()[0]}</b>\n\n"
                if len(gemini_output_escaped.splitlines()) > 1:
                    message_part += "\n".join(gemini_output_escaped.splitlines()[1:])
                
                message_part += f"\n\nğŸ”— <a href='{html_lib.escape(entry.link)}'>Ù„ÛŒÙ†Ú© Ù…Ù‚Ø§Ù„Ù‡ Ø§ØµÙ„ÛŒ</a>"

                if send_telegram_message(message_part):
                    last_sent_links[url] = entry_id
                    logging.info(f"Ù…Ù‚Ø§Ù„Ù‡ Ø¬Ø¯ÛŒØ¯ Ø§Ø±Ø³Ø§Ù„ Ø´Ø¯: {title}")
                    time.sleep(5)
                else:
                    logging.error(f"Ø§Ø±Ø³Ø§Ù„ Ù¾ÛŒØ§Ù… Ø¨Ø±Ø§ÛŒ Ù…Ù‚Ø§Ù„Ù‡ {title} Ù†Ø§Ù…ÙˆÙÙ‚ Ø¨ÙˆØ¯.")
                    break
        except Exception as e:
            logging.error(f"Ø®Ø·Ø§ÛŒ Ø¬Ø¯ÛŒ Ø¯Ø± Ù¾Ø±Ø¯Ø§Ø²Ø´ ÙÛŒØ¯ {url}: {e}")
            continue
    
    save_data({"last_sent_links": last_sent_links})
    logging.info("Ù¾Ø§ÛŒØ§Ù† ÛŒÚ© Ú†Ø±Ø®Ù‡ Ø¨Ø±Ø±Ø³ÛŒ.")

if __name__ == "__main__":
    check_news_job()
