# -*- coding: utf-8 -*-
import os
import logging
import json
import re
import requests
import time
import feedparser
import google.generativeai as genai
from urllib.parse import quote
import schedule

# --- Ø®ÙˆØ§Ù†Ø¯Ù† Ù…ØªØºÛŒØ±Ù‡Ø§ÛŒ Ù…Ø­Ø±Ù…Ø§Ù†Ù‡ Ø§Ø² Ù…Ø­ÛŒØ· Railway ---
TELEGRAM_BOT_TOKEN = os.environ.get("TELEGRAM_BOT_TOKEN")
ADMIN_CHAT_ID = os.environ.get("ADMIN_CHAT_ID")
GEMINI_API_KEY = os.environ.get("GEMINI_API_KEY")
ADMIN_NAME = "Ø¬Ù†Ø§Ø¨ Ø±ÙÛŒØ¹ÛŒ"

# --- Ù¾ÛŒÚ©Ø±Ø¨Ù†Ø¯ÛŒ Ø¬Ù…ÛŒÙ†Ø§ÛŒ ---
# Ø§ÛŒÙ† Ø¨Ø®Ø´ Ø¨Ø§ÛŒØ¯ Ø¨Ø¹Ø¯ Ø§Ø² Ø®ÙˆØ§Ù†Ø¯Ù† Ù…ØªØºÛŒØ±Ù‡Ø§ Ø¨Ø§Ø´Ø¯
if GEMINI_API_KEY:
    try:
        genai.configure(api_key=GEMINI_API_KEY)
        model = genai.GenerativeModel('gemini-pro')
    except Exception as e:
        logging.error(f"Ø®Ø·Ø§ Ø¯Ø± Ù¾ÛŒÚ©Ø±Ø¨Ù†Ø¯ÛŒ Ø¬Ù…ÛŒÙ†Ø§ÛŒ: {e}")
else:
    logging.error("Ú©Ù„ÛŒØ¯ API Ø¬Ù…ÛŒÙ†Ø§ÛŒ Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯!")

# --- Ù…Ø³ÛŒØ± ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ ---
DB_FILE = "/tmp/bot_database.json" # Ø§Ø² Ø­Ø§ÙØ¸Ù‡ Ù…ÙˆÙ‚Øª Railway Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…
URL_FILE = "urls.txt"

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')

KEYWORD_CATEGORIES = {
    "ğŸ”µ": ['Ù†Ø¬ÙˆÙ…', 'ÙÛŒØ²ÛŒÚ©', 'Ú©ÛŒÙ‡Ø§Ù†', 'Ú©ÙˆØ§Ù†ØªÙˆÙ…', 'Ø³ØªØ§Ø±Ù‡', 'Ú©Ù‡Ú©Ø´Ø§Ù†', 'Ø³ÛŒØ§Ù‡Ú†Ø§Ù„Ù‡', 'Ø§Ø®ØªØ±Ø´Ù†Ø§Ø³ÛŒ', 'Ø³ÛŒØ§Ø±Ù‡', 'physics', 'astronomy', 'cosmos', 'galaxy', 'planet'],
    "ğŸŸ¡": ['Ø²ÛŒØ³Øª', 'Ú˜Ù†ØªÛŒÚ©', 'ÙØ±Ú¯Ø´Øª', 'dna', 'Ø³Ù„ÙˆÙ„', 'Ù…ÙˆÙ„Ú©ÙˆÙ„', 'Ø¨ÛŒÙˆÙ„ÙˆÚ˜ÛŒ', 'ØªÚ©Ø§Ù…Ù„', 'biology', 'evolution', 'genetic'],
    "âš«": ['Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ', 'ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ù…Ø§Ø´ÛŒÙ†', 'Ø´Ø¨Ú©Ù‡ Ø¹ØµØ¨ÛŒ', 'Ø±Ø¨Ø§ØªÛŒÚ©', 'Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ…', 'Ø¯ÛŒÙ¾ Ù„Ø±Ù†ÛŒÙ†Ú¯', 'ai', 'artificial intelligence', 'machine learning'],
    "ğŸ”´": ['Ø±ÙˆØ§Ù†Ø´Ù†Ø§Ø³ÛŒ', 'Ø¬Ø§Ù…Ø¹Ù‡ Ø´Ù†Ø§Ø³ÛŒ', 'Ø¹Ù„ÙˆÙ… Ø§Ø¬ØªÙ…Ø§Ø¹ÛŒ', 'Ø±ÙØªØ§Ø±', 'Ø°Ù‡Ù†', 'Ø±ÙˆØ§Ù†', 'Ø§Ø¬ØªÙ…Ø§Ø¹ÛŒ', 'psychology', 'sociology', 'social'],
    "ğŸŸ ": ['ÙÙ„Ø³ÙÙ‡', 'ÙÙ„Ø³ÙÙ‡ Ø¹Ù„Ù…', 'Ù…Ù†Ø·Ù‚', 'Ù…ØªØ§ÙÛŒØ²ÛŒÚ©', 'Ø§Ø®Ù„Ø§Ù‚', 'philosophy']
}

def load_data():
    try:
        with open(DB_FILE, 'r') as f:
            return json.load(f)
    except FileNotFoundError:
        return {"last_sent_links": {}}

def save_data(data):
    with open(DB_FILE, 'w') as f:
        json.dump(data, f, indent=4)

def clean_html(raw_html):
    return re.sub(re.compile('<.*?>'), '', raw_html)

def categorize_article(text):
    text_lower = text.lower()
    emojis = ""
    for emoji, keywords in KEYWORD_CATEGORIES.items():
        if any(keyword in text_lower for keyword in keywords):
            emojis += emoji
    return emojis if emojis else "ğŸ“°"

def send_telegram_message(text):
    encoded_text = quote(text)
    url = f"https://api.telegram.org/bot{TELEGRAM_BOT_TOKEN}/sendMessage?chat_id={ADMIN_CHAT_ID}&text={encoded_text}&parse_mode=Markdown"
    try:
        requests.get(url, timeout=10)
    except Exception as e:
        logging.error(f"Ø®Ø·Ø§ Ø¯Ø± Ø§Ø±Ø³Ø§Ù„ Ù¾ÛŒØ§Ù… ØªÙ„Ú¯Ø±Ø§Ù…: {e}")

def process_with_gemini(title, summary):
    try:
        prompt = (
            "You are an expert science communicator. Your task is to perform two steps based on the following English text:\n"
            "1. Provide a fluent and professional Persian translation of ONLY the title.\n"
            "2. After the title, explain the core concept of the article in a few simple and conceptual Persian sentences, as if you are explaining it to an expert student (like Ø´Ø§Ú¯Ø±Ø¯Ù… in Persian).\n\n"
            f"Title: '{title}'\n"
            f"Summary: '{summary}'\n\n"
            "Your final output must be in Persian and structured exactly like this:\n"
            "[Persian Title]\n\n"
            "[Simple and conceptual Persian explanation]"
        )
        response = model.generate_content(prompt)
        return response.text
    except Exception as e:
        logging.error(f"Ø®Ø·Ø§ Ø¯Ø± Ø§Ø±ØªØ¨Ø§Ø· Ø¨Ø§ Ø¬Ù…ÛŒÙ†Ø§ÛŒ: {e}")
        return f"{title}\n\n(Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¨Ø§ Ø¬Ù…ÛŒÙ†Ø§ÛŒ Ù†Ø§Ù…ÙˆÙÙ‚ Ø¨ÙˆØ¯)"

def check_news_job():
    database = load_data()
    last_sent_links = database.get("last_sent_links", {})
    
    logging.info("Ø´Ø±ÙˆØ¹ Ú†Ø±Ø®Ù‡ Ø¨Ø±Ø±Ø³ÛŒ Ø§Ø®Ø¨Ø§Ø±...")
    try:
        with open(URL_FILE, 'r') as f:
            urls = [line.strip() for line in f if line.strip() and not line.startswith('#')]
    except FileNotFoundError:
        logging.warning("ÙØ§ÛŒÙ„ urls.txt Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯!")
        urls = []
for url in urls:
        logging.info(f"Ø¯Ø± Ø­Ø§Ù„ Ø¨Ø±Ø±Ø³ÛŒ Ø³Ø§ÛŒØª: {url}")
        new_articles_found = []
        try:
            feed = feedparser.parse(url)
            if not feed or not feed.entries:
                logging.warning(f"ÙÛŒØ¯ Ø¨Ø±Ø§ÛŒ Ø³Ø§ÛŒØª {url} Ø®Ø§Ù„ÛŒ ÛŒØ§ Ù†Ø§Ù…Ø¹ØªØ¨Ø± Ø§Ø³Øª.")
                continue
            
            for entry in reversed(feed.entries[:15]): # ØªØ§ Û±Ûµ Ù…Ù‚Ø§Ù„Ù‡ Ø¢Ø®Ø± Ù‡Ø± ÙÛŒØ¯ Ø±Ø§ Ú†Ú© Ù…ÛŒâ€ŒÚ©Ù†Ø¯
                entry_link = entry.get('id', entry.link) # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² ID Ø¯Ø± ØµÙˆØ±Øª ÙˆØ¬ÙˆØ¯
                
                if last_sent_links.get(url) != entry_link:
                    title = entry.title
                    summary = clean_html(entry.summary)
                    full_content_for_cat = f"Title: {title}. Summary: {summary}"
                    emojis = categorize_article(full_content_for_cat)
                    gemini_output = process_with_gemini(title, summary)
                    message_part = f"{emojis} *{gemini_output}*\n\n[Ù„ÛŒÙ†Ú© Ù…Ù‚Ø§Ù„Ù‡ Ø§ØµÙ„ÛŒ]({entry.link})"
                    new_articles_found.append(message_part)
                    
                    logging.info(f"Ù…Ù‚Ø§Ù„Ù‡ Ø¬Ø¯ÛŒØ¯ Ù¾ÛŒØ¯Ø§ Ø´Ø¯: {title}")
                else:
                    # ÙˆÙ‚ØªÛŒ Ø¨Ù‡ Ø¢Ø®Ø±ÛŒÙ† Ù…Ù‚Ø§Ù„Ù‡ Ø§Ø±Ø³Ø§Ù„ Ø´Ø¯Ù‡ Ø±Ø³ÛŒØ¯ÛŒÙ…ØŒ Ø¨Ù‚ÛŒÙ‡ Ù‚Ø¯ÛŒÙ…ÛŒâ€ŒØªØ± Ù‡Ø³ØªÙ†Ø¯
                    break

            # Ø§Ø±Ø³Ø§Ù„ Ù…Ù‚Ø§Ù„Ø§Øª Ù¾ÛŒØ¯Ø§ Ø´Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ Ø§ÛŒÙ† Ø³Ø§ÛŒØª
            if new_articles_found:
                header = f"ğŸ“¬ **Ø§Ø®Ø¨Ø§Ø± Ø¬Ø¯ÛŒØ¯ Ø§Ø² Ø³Ø§ÛŒØª {url.split('//')[1].split('/')[0]} Ø¨Ø±Ø§ÛŒ Ø´Ù…Ø§ØŒ {ADMIN_NAME} Ø¹Ø²ÛŒØ²:**\n\n---"
                send_telegram_message(header)
                for article_text in new_articles_found:
                    send_telegram_message(article_text)
                    time.sleep(3)
            
            # Ø¨Ø±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ø¢Ø®Ø±ÛŒÙ† Ù„ÛŒÙ†Ú© Ø¯ÛŒØ¯Ù‡ Ø´Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ Ø§ÛŒÙ† Ø³Ø§ÛŒØª
            if feed.entries:
                last_sent_links[url] = feed.entries[0].get('id', feed.entries[0].link)

        except Exception as e:
            logging.error(f"Ø®Ø·Ø§ÛŒ Ø¬Ø¯ÛŒ Ø¯Ø± Ù¾Ø±Ø¯Ø§Ø²Ø´ ÙÛŒØ¯ {url}: {e}")
            continue
    
    # Ø°Ø®ÛŒØ±Ù‡ Ù†Ù‡Ø§ÛŒÛŒ Ø¯ÛŒØªØ§Ø¨ÛŒØ³ Ø¨Ø¹Ø¯ Ø§Ø² Ø¨Ø±Ø±Ø³ÛŒ Ù‡Ù…Ù‡ Ø³Ø§ÛŒØªâ€ŒÙ‡Ø§
    save_data({"last_sent_links": last_sent_links})
    logging.info("Ù¾Ø§ÛŒØ§Ù† ÛŒÚ© Ú†Ø±Ø®Ù‡ Ø¨Ø±Ø±Ø³ÛŒ.")

if name == "__main__":
    logging.info("Ø±Ø¨Ø§Øª Ø¯Ø± Ø­Ø§Ù„ Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ø¨Ø±Ø§ÛŒ Ø§Ø¬Ø±Ø§ÛŒ Ø¯Ø§Ø¦Ù…ÛŒ Ø§Ø³Øª...")
    # Ø²Ù…Ø§Ù†â€ŒØ¨Ù†Ø¯ÛŒ Ø¨Ø±Ø§ÛŒ Ø§Ø¬Ø±Ø§ Ù‡Ø± Û´ Ø³Ø§Ø¹Øª
    schedule.every(4).hours.do(check_news_job)
    
    # Ø§Ø¬Ø±Ø§ÛŒ Ø§ÙˆÙ„ÛŒÙ‡ Ø¨Ø±Ø§ÛŒ ØªØ³Øª Ø¯Ø± Ø´Ø±ÙˆØ¹ Ú©Ø§Ø±
    check_news_job()
    
    while True:
        schedule.run_pending()
        time.sleep(60) # Ù‡Ø± Ø¯Ù‚ÛŒÙ‚Ù‡ ÛŒÚ©â€ŒØ¨Ø§Ø± Ú†Ú© Ù…ÛŒâ€ŒÚ©Ù†Ø¯ Ú©Ù‡ Ø¢ÛŒØ§ Ø²Ù…Ø§Ù† Ø§Ø¬Ø±Ø§ÛŒ ÙˆØ¸ÛŒÙÙ‡ Ø±Ø³ÛŒØ¯Ù‡ ÛŒØ§ Ù†Ù‡
